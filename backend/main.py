"""
SLATE Backend - Simplified Wallet & DeFi Assistant
=================================================

OVERVIEW:
This is a simplified, production-ready backend for a blockchain wallet and DeFi assistant.
It follows a clean "Plan ‚Üí Execute ‚Üí Summarize" pattern with memory management.

ARCHITECTURE:
============
üìã PLANNING: LLM analyzes user intent and creates tool execution plan
‚öôÔ∏è EXECUTION: Backend tools execute immediately, frontend tools queued
üìÑ SUMMARIZATION: LLM generates final user response with conversation memory
üíæ MEMORY: Maintains conversation context and user preferences

SIMPLIFIED TOOLS (4 total):
==========================
Frontend Tools:
- wallet_connect: Check TronLink and connect wallet
- wallet_check: Check connection status and fetch balance

Backend Tools:
- justlend_markets: List JustLend markets with APY data
- justlend_market_detail: Get specific market information

API ENDPOINTS:
=============
POST /api/chat - Main chat endpoint (planning phase)
POST /api/chat/summarize - Generate final user response
POST /api/tools/report - Store tool results (optional)
POST /api/wallet/* - Wallet state management
GET /health - System status

ENVIRONMENT SETUP:
=================
OPENAI_API_KEY=your_openai_key (Required)
OPENAI_MODEL=gpt-4o-mini (Optional, defaults to gpt-4o-mini)
TRON_NETWORK=mainnet|nile (Optional, defaults to mainnet)  
TRONGRID_API_KEY=your_trongrid_key (Required for mainnet)
JL_UNITROLLER_NILE=contract_address (Required for nile network)

DEPLOYMENT NOTES:
================
- All user-facing text generated by LLM (no templates)
- Conversation memory stored in-memory (consider Redis for production)
- Modular architecture for easy maintenance
- Comprehensive logging for debugging
- CORS configured for frontend development
"""

import os
import time
import json
import random
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
from dotenv import load_dotenv

# LLM (OpenAI SDK v1.x)
from openai import OpenAI

# Local modules
from models import ChatMessage, ChatResponse, WalletConnected, WalletError, WalletDetails
from tron_client import create_tron_client
from justlend_ops import list_markets, market_detail, user_position
from llm_planner import TOOL_SPEC, update_conversation_memory, plan_with_llm, summarize_with_llm, decide_widget

# -----------------------------------------------------------------------------
# Environment Configuration & Startup
# -----------------------------------------------------------------------------
print("üöÄ [STARTUP] Loading SLATE Backend...")
print("üöÄ [STARTUP] Reading environment variables...")

load_dotenv()

# Environment variable extraction with validation
TRON_NETWORK = os.getenv("TRON_NETWORK", "mainnet").lower()   # mainnet | nile
TRONGRID_API_KEY = os.getenv("TRONGRID_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

print(f"üåê [CONFIG] TRON_NETWORK: {TRON_NETWORK}")
print(f"üß† [CONFIG] OPENAI_MODEL: {OPENAI_MODEL}")
print(f"üåê [CONFIG] TRONGRID_API_KEY: {'‚úÖ Set' if TRONGRID_API_KEY else '‚ùå Missing'}")
print(f"üß† [CONFIG] OPENAI_API_KEY: {'‚úÖ Set' if OPENAI_API_KEY else '‚ùå Missing'}")

# Validation
if TRON_NETWORK == "mainnet" and not TRONGRID_API_KEY:
    print("‚ö†Ô∏è [ERROR] TRONGRID_API_KEY is required for mainnet access")
    raise RuntimeError("TRONGRID_API_KEY is required when TRON_NETWORK=mainnet")

if not OPENAI_API_KEY:
    print("‚ö†Ô∏è [ERROR] OPENAI_API_KEY is missing")
    raise RuntimeError("OPENAI_API_KEY missing")

print("üß† [STARTUP] Initializing OpenAI client...")
client_llm = OpenAI(api_key=OPENAI_API_KEY)
print("‚úÖ [STARTUP] OpenAI client initialized successfully")

# -----------------------------------------------------------------------------
# FastAPI Application Setup
# -----------------------------------------------------------------------------
print("üöÄ [STARTUP] Configuring FastAPI application...")

app = FastAPI(title="SLATE Backend", version="12.0.0")

# In main.py, update the CORS section:
allowed_origins = [
    "http://localhost:5173", "http://127.0.0.1:5173",  # Vite default
    "http://localhost:3000", "http://127.0.0.1:3000",  # React default
    "http://localhost:3001", "http://127.0.0.1:3001",  # Alternative port
    "https://slate-frontend.onrender.com",  # Your Render frontend URL
    "https://*.onrender.com",  # Allow any Render subdomain
]

print(f"üåê [CORS] Allowed origins: {allowed_origins}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

print("‚úÖ [STARTUP] FastAPI configured with CORS middleware")

# -----------------------------------------------------------------------------
# In-Memory State Management
# -----------------------------------------------------------------------------
print("üíæ [STARTUP] Initializing in-memory storage...")

# Session storage: chat history, user profiles, wallet state
sessions: Dict[str, Dict[str, Any]] = {}
print("üíæ [STORAGE] Sessions dictionary initialized")

# Tool result storage: for summarization and debugging
last_tool_results: Dict[str, Dict[str, Any]] = {}
print("üíæ [STORAGE] Tool results storage initialized")

print("‚úÖ [STARTUP] Memory systems ready")

# Note: TRON client and JustLend operations moved to separate modules

# Note: LLM planning logic moved to llm_planner.py module

def execute_backend_calls(plan: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Execute backend tool calls immediately, pass through frontend calls.
    
    Args:
        plan: List of planned function calls
        
    Returns:
        List of function calls with results for backend tools
    """
    print(f"‚öôÔ∏è [EXECUTION] Starting backend execution for {len(plan)} planned calls")
    
    out: List[Dict[str, Any]] = []
    backend_calls = 0
    frontend_calls = 0
    
    for i, step in enumerate(plan):
        t = step.get("type")
        args = (step.get("args") or {})
        
        print(f"‚öôÔ∏è [EXECUTION] Processing step {i+1}: {t}")
        
        if t == "trustlender_list_markets":
            backend_calls += 1
            print(f"‚öôÔ∏è [EXECUTION] Executing backend call: {t}")
            
            lim = int(args.get("limit", 6))
            try:
                data = list_markets(lim)
                out.append({"type": t, "args": {"limit": lim}, "result": data, "executed": "backend"})
                print(f"‚úÖ [EXECUTION] Backend call {t} completed successfully")
            except Exception as e:
                print(f"‚ùå [EXECUTION] Backend call {t} failed: {e}")
                out.append({"type": t, "args": {"limit": lim}, "error": str(e), "executed": "backend"})
                
        elif t == "trustlender_market_detail":
            backend_calls += 1
            print(f"‚öôÔ∏è [EXECUTION] Executing backend call: {t}")
            
            sym = str(args.get("symbol") or "").upper()
            if not sym:
                print(f"‚ö†Ô∏è [EXECUTION] Missing required symbol for {t}")
                out.append({"type": t, "args": args, "error": "symbol required", "executed": "backend"})
            else:
                try:
                    data = market_detail(sym)
                    out.append({"type": t, "args": {"symbol": sym}, "result": data, "executed": "backend"})
                    print(f"‚úÖ [EXECUTION] Backend call {t} completed successfully")
                except Exception as e:
                    print(f"‚ùå [EXECUTION] Backend call {t} failed: {e}")
                    out.append({"type": t, "args": {"symbol": sym}, "error": str(e), "executed": "backend"})
                    
        elif t == "trustlender_user_position":
            backend_calls += 1
            print(f"‚öôÔ∏è [EXECUTION] Executing backend call: {t}")
            
            addr = args.get("address")
            if not addr:
                print(f"‚ö†Ô∏è [EXECUTION] Missing required address for {t}")
                out.append({"type": t, "args": args, "error": "address required", "executed": "backend"})
            else:
                try:
                    data = user_position(addr)
                    out.append({"type": t, "args": {"address": addr}, "result": data, "executed": "backend"})
                    print(f"‚úÖ [EXECUTION] Backend call {t} completed successfully")
                except Exception as e:
                    print(f"‚ùå [EXECUTION] Backend call {t} failed: {e}")
                    out.append({"type": t, "args": {"address": addr}, "error": str(e), "executed": "backend"})
        else:
            # wallet_* steps are executed on the frontend
            frontend_calls += 1
            print(f"‚öôÔ∏è [EXECUTION] Passing through frontend call: {t}")
            out.append(step)
    
    print(f"‚úÖ [EXECUTION] Completed: {backend_calls} backend calls, {frontend_calls} frontend calls")
    return out

# -----------------------------------------------------------------------------
# API Endpoint: /api/chat - Planning Phase
# -----------------------------------------------------------------------------
@app.post("/api/chat", response_model=ChatResponse)
def api_chat(msg: ChatMessage):
    """
    Main chat endpoint - Planning phase only.
    
    Process:
    1. Analyze user intent using LLM
    2. Generate execution plan
    3. Execute backend tools immediately
    4. Return plan to frontend (reply is empty)
    
    Args:
        msg: ChatMessage with user query and session_id
        
    Returns:
        ChatResponse with empty reply and function_calls array
    """
    session_id = msg.session_id or f"session_{int(time.time())}"
    text = msg.message or ""
    
    print(f"\nüìã [API] /api/chat called")
    print(f"üìã [API] Session: {session_id}")
    print(f"üìã [API] Message: {text}")

    # Update session history
    s = sessions.setdefault(session_id, {"chat_history": [], "profile": {}})
    s["chat_history"].append({"role": "human", "content": text})
    print(f"üíæ [API] Updated chat history (total: {len(s['chat_history'])} messages)")

    # Generate execution plan using LLM
    print("üìã [API] Starting planning phase...")
    plan = plan_with_llm(client_llm, session_id, text, OPENAI_MODEL)
    print(f"üìã [API] Plan generated with {len(plan)} steps")
    
    # Execute backend calls immediately
    print("‚öôÔ∏è [API] Starting execution phase...")
    calls = execute_backend_calls(plan)
    print(f"‚öôÔ∏è [API] Execution completed, returning {len(calls)} function calls")

    # Handle response generation and widget decision
    reply = ""
    widget_info = {"type": "idle", "data": None}  # Default widget
    
    if len(calls) == 0:
        # No tools were planned - generate conversational response
        print("üí¨ [API] No tools planned - generating conversational response")
        reply = summarize_with_llm(client_llm, text, "no_tool", {}, session_id, OPENAI_MODEL)
        update_conversation_memory(session_id, text, reply, "no_tool", None)
        print(f"üí¨ [API] Generated conversational response: {reply}")
        # Keep idle widget for conversational responses
    else:
        # Check if any backend calls were completed (successful or failed)
        backend_calls = [call for call in calls if call.get("executed") == "backend"]
        if backend_calls:
            # Auto-summarize the last backend call result
            last_backend = backend_calls[-1]  # Take the last backend call
            tool_name = last_backend.get("type")
            tool_result = last_backend.get("result") or last_backend.get("error", "Unknown error")
            
            print(f"üí¨ [API] Auto-summarizing backend result for: {tool_name}")
            reply = summarize_with_llm(client_llm, text, tool_name, tool_result, session_id, OPENAI_MODEL)
            update_conversation_memory(session_id, text, reply, tool_name, tool_result)
            print(f"üí¨ [API] Generated backend summary: {reply}")
            
            # Decide which widget to show based on tool and result
            widget_info = decide_widget(tool_name, tool_result, session_id)

    # Return plan to frontend with widget information
    response = ChatResponse(
        reply=reply,  # Include response for no-tool cases and backend calls
        function_calls=calls,
        widget=widget_info,  # Widget decision from summarizer
        session_id=session_id,
        timestamp=datetime.now().isoformat(),
    )
    
    print(f"‚úÖ [API] /api/chat completed successfully")
    return response

# -----------------------------------------------------------------------------
# API Endpoint: /api/tools/report - Tool Result Storage
# -----------------------------------------------------------------------------
@app.post("/api/tools/report")
def api_tools_report(payload: Dict[str, Any] = Body(...)):
    """
    Store tool execution results for summarization and debugging.
    
    Args:
        payload: Dict containing session_id and result data
        
    Returns:
        Dict with success status
    """
    session_id = payload.get("session_id")
    result = payload.get("result")
    
    print(f"\nüíæ [REPORT] /api/tools/report called")
    print(f"üíæ [REPORT] Session: {session_id}")
    print(f"üíæ [REPORT] Result keys: {list(result.keys()) if result else 'None'}")

    if not session_id:
        print("‚ö†Ô∏è [REPORT] Missing session_id")
        return {"ok": False, "error": "missing session_id"}

    tool_name = (result or {}).get("tool", "unknown_tool")
    print(f"üíæ [REPORT] Tool name: {tool_name}")
    
    # Store in tool results for summarizer
    last_tool_results.setdefault(session_id, {})[tool_name] = result or {}
    print(f"üíæ [REPORT] Stored result in last_tool_results")

    # Also store in session for auditing/debugging
    s = sessions.setdefault(session_id, {"chat_history": [], "profile": {}})
    s.setdefault("last_tools", {})[tool_name] = result or {}
    print(f"üíæ [REPORT] Stored result in session data")
    
    print("‚úÖ [REPORT] Tool result stored successfully")
    return {"ok": True}

# -----------------------------------------------------------------------------
# Summarizer - Final Response Generation
# -----------------------------------------------------------------------------
# Note: Summarizer function moved to llm_planner.py module

@app.post("/api/chat/summarize")
def api_chat_summarize(payload: Dict[str, Any] = Body(...)):
    """
    Generate final user-facing response based on tool results.
    
    This is the END OF GRAPH - the only place user text is created.
    
    Args:
        payload: Dict with session_id, tool name, and optional result
        
    Returns:
        Dict with AI-generated reply
    """
    session_id = payload.get("session_id")
    tool = payload.get("tool")
    provided_result = payload.get("result")
    
    print(f"\nüìÑ [API] /api/chat/summarize called")
    print(f"üìÑ [API] Session: {session_id}")
    print(f"üìÑ [API] Tool: {tool}")
    print(f"üìÑ [API] Has provided result: {provided_result is not None}")

    if not session_id:
        print("‚ö†Ô∏è [API] Missing session_id")
        return {"reply": ""}

    # Retrieve original question (last human message)
    question = ""
    hist = sessions.setdefault(session_id, {"chat_history": [], "profile": {}})["chat_history"]
    
    print(f"üìÑ [API] Searching through {len(hist)} chat history items...")
    for item in reversed(hist):
        if item["role"] == "human":
            question = item["content"]
            print(f"üìÑ [API] Found original question: {question}")
            break
    
    if not question:
        print("‚ö†Ô∏è [API] No original question found in history")

    # Get result either from request or memory
    result = provided_result or last_tool_results.get(session_id, {}).get(tool or "", {})
    print(f"üìÑ [API] Using result from: {'provided payload' if provided_result else 'memory'}")

    # Generate the *only* user-facing text dynamically
    print("üìÑ [API] Starting LLM summarization...")
    reply = summarize_with_llm(client_llm, question, tool or "", result or {}, session_id, OPENAI_MODEL)

    # Update LangChain conversation memory with tool result context
    update_conversation_memory(session_id, question, reply, tool or "", result)

    # Decide which widget to show for frontend tools too
    widget_info = decide_widget(tool or "", result or {}, session_id)

    # Store in chat history for continuity/debug
    hist.append({"role": "ai", "content": reply})
    print(f"üíæ [API] Added AI response to chat history")
    print(f"‚úÖ [API] /api/chat/summarize completed")
    
    return {"reply": reply, "widget": widget_info}

# -----------------------------------------------------------------------------
# Wallet Memory Endpoints - Session State Only
# -----------------------------------------------------------------------------
@app.post("/api/wallet/connected")
def wallet_connected(evt: WalletConnected):
    """
    Store wallet connection state in session memory.
    No user-facing messages generated here.
    """
    print(f"\nüíæ [WALLET] Connection event: {evt.address}")
    print(f"üíæ [WALLET] Network: {evt.network}, Host: {evt.node_host}")
    
    s = sessions.setdefault(evt.session_id, {"chat_history": [], "profile": {}})
    s["wallet"] = {
        "address": evt.address,
        "network": evt.network,
        "node_host": evt.node_host,
        "connected_at": datetime.now().isoformat()
    }
    s["profile"]["wallet_connected"] = True
    s["profile"]["wallet_address"] = evt.address
    
    print("‚úÖ [WALLET] Connection state stored")
    return {"ok": True}

@app.post("/api/wallet/details")
def wallet_details(evt: WalletDetails):
    """
    Store wallet details (balance, etc.) in session memory.
    No user-facing messages generated here.
    """
    print(f"\nüíæ [WALLET] Details update: {evt.address}")
    print(f"üíæ [WALLET] Balance: {evt.trx_balance}")
    
    s = sessions.setdefault(evt.session_id, {"chat_history": [], "profile": {}})
    s["wallet_details"] = evt.dict()
    s["profile"]["trx_balance"] = evt.trx_balance
    s["profile"]["trx_balance_updated_at"] = datetime.now().isoformat()
    
    print("‚úÖ [WALLET] Details stored")
    return {"ok": True}

@app.post("/api/wallet/error")
def wallet_error(evt: WalletError):
    """
    Store wallet error in session memory.
    No user-facing messages generated here.
    """
    print(f"\n‚ö†Ô∏è [WALLET] Error event: {evt.error}")
    
    s = sessions.setdefault(evt.session_id, {"chat_history": [], "profile": {}})
    s["last_wallet_error"] = {"error": evt.error, "ts": datetime.now().isoformat()}
    
    print("üíæ [WALLET] Error stored")
    return {"ok": True}

@app.get("/health")
def health():
    """
    Health check endpoint for monitoring and debugging.
    
    Returns:
        Dict with system status and configuration info
    """
    print("üîç [HEALTH] Health check requested")
    
    return {
        "status": "ok", 
        "time": datetime.now().isoformat(),
        "version": "12.0.0",
        "network": TRON_NETWORK,
        "model": OPENAI_MODEL,
        "session_count": len(sessions),
        "tool_result_count": len(last_tool_results)
    }

# -----------------------------------------------------------------------------
# Application Entrypoint
# -----------------------------------------------------------------------------
# At the end of main.py, replace the existing if __name__ == "__main__": block with:

if __name__ == "__main__":
    import os
    
    # Get port from environment (Render provides this)
    port = int(os.getenv("PORT", 8000))
    host = "0.0.0.0"  # Important: must be 0.0.0.0 for Render
    
    print("üöÄ [STARTUP] Starting SLATE Backend Server...")
    print(f"üöÄ [STARTUP] Host: {host}:{port}")
    print("üöÄ [STARTUP] Reload: Disabled for production")
    print("‚úÖ [STARTUP] Server ready!")
    
    # Production mode - no reload
    uvicorn.run("main:app", host=host, port=port, reload=False)
